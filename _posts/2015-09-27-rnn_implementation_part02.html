--- 
layout: notebook_simple_rnn_post 
title: How to implement a recurrent neural network Part 2 
---


<html>
 <body>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Binary-addition-with-a-non-linear-RNN">
      Binary addition with a non-linear RNN
      <a class="anchor-link" href="#Binary-addition-with-a-non-linear-RNN">
       ¶
      </a>
     </h2>
     <p>
      This part will cover:
     </p>
     <ul>
      <li>
       Store data in
       <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/#Dataset">
        tensor
       </a>
      </li>
      <li>
       Optimization with
       <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/#Rmsprop-with-momentum-optimisation">
        Rmsprop and Nesterov momentum
       </a>
      </li>
     </ul>
     <p>
      While the
      <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
       first part
      </a>
      of this tutorial described a simple linear recurrent network, this tutorial will describe an RNN with non-linear transfer functions that is able to learn how to perform
      <a href="https://en.wikipedia.org/wiki/Binary_number#Addition">
       binary addition
      </a>
      from examples.
     </p>
     <p>
      First we import the libraries we need and define the dataset.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [1]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Python imports</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c"># Matrix and vector computation package</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>  <span class="c"># Plotting library</span>
<span class="c"># Allow matplotlib to plot inside this notebook</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c"># Set the seed of the numpy random number generator so that the tutorial is reproducable</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Dataset">
      Dataset
      <a class="anchor-link" href="#Dataset">
       ¶
      </a>
     </h2>
     <p>
      This tutorial uses a dataset of 2000 training samples to train the RNN that can be created with the
      <code>
       create_dataset
      </code>
      method defined below. Each sample consists of two 6-bit input numbers ($x_{i1}$, $x_{i2}$) padded with a 0 to make it 7 characters long, and a 7-bit target number ($t_{i}$) so that $t_{i} = x_{i1} + x_{i2}$ ($i$ is the sample index). The numbers are represented as
      <a href="https://en.wikipedia.org/wiki/Binary_number">
       binary numbers
      </a>
      with the
      <a href="https://en.wikipedia.org/wiki/Most_significant_bit">
       most significant bit
      </a>
      on the right (least significant bit first). This is so that our RNN can perform the addition form left to right.
     </p>
     <p>
      The input and target vectors are stored in a 3th-order tensor. A
      <a href="https://en.wikipedia.org/wiki/Tensor">
       tensor
      </a>
      is a generalisation of vectors and matrices, a vector is a 1st-order tensor, a matrix is a 2nd-order tensor. The order of a tensor is the dimensionality of the array data structure needed to represent it.
      <br/>
      The dimensions of our training data (
      <code>
       X_train
      </code>
      ,
      <code>
       T_train
      </code>
      ) are printed after the creation of the dataset below. The first order of our data tensors goes over all the samples (2000 samples), the second order goes over the variables per unit of time (7 timesteps), and the third order goes over the variables for each timestep and sample (e.g. input variables $x_{ik1}$, $x_{ik2}$ with $i$ the sample index and $k$ the timestep). The input tensor
      <code>
       X_train
      </code>
      is visualised in the following figure:
     </p>
     <p>
      <img alt="Visualisation of input tensor X" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_Tensor.png"/>
     </p>
     <p>
      The following code block initialises the dataset.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [2]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Create dataset</span>
<span class="n">nb_train</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c"># Number of training samples</span>
<span class="c"># Addition of 2 n-bit numbers can result in a n+1 bit number</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c"># Length of the binary sequence</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">):</span>
    <span class="sd">"""Create a dataset for binary addition and return as input, targets."""</span>
    <span class="n">max_int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c"># Maximum integer that can be added</span>
    <span class="n">format_str</span> <span class="o">=</span> <span class="s">'{:0'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">)</span> <span class="o">+</span> <span class="s">'b}'</span> <span class="c"># Transform integer in binary format</span>
    <span class="n">nb_inputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c"># Add 2 binary numbers</span>
    <span class="n">nb_outputs</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c"># Result is 1 binary number</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_inputs</span><span class="p">))</span>  <span class="c"># Input samples</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_outputs</span><span class="p">))</span>  <span class="c"># Target samples</span>
    <span class="c"># Fill up the input and target matrix</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">):</span>
        <span class="c"># Generate random numbers to add</span>
        <span class="n">nb1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="n">nb2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="c"># Fill current input and target row.</span>
        <span class="c"># Note that binary numbers are added from right to left, but our RNN reads </span>
        <span class="c">#  from left to right, so reverse the sequence.</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="p">)]))</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb2</span><span class="p">)]))</span>
        <span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="o">+</span><span class="n">nb2</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span>

<span class="c"># Create training samples</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">T_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_train</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'X_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'T_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">T_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre>
      </div>
     </div>
    </div>
   </div>
   <div class="output_wrapper">
    <div class="output">
     <div class="output_area">
      <div class="prompt">
      </div>
      <div class="output_subarea output_stream output_stdout output_text">
       <pre>X_train shape: (2000, 7, 2)
T_train shape: (2000, 7, 1)
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h3 id="Binary-addition">
      Binary addition
      <a class="anchor-link" href="#Binary-addition">
       ¶
      </a>
     </h3>
     <p>
      Performing binary addition is a good
      <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf">
       toy problem
      </a>
      to illustrate how recurrent neural networks process input streams into output streams. The network needs to learn how to carry a bit to the next state (memory) and when to output a 0 or 1 dependent on the input and state.
     </p>
     <p>
      The following code prints a visualisation of the inputs and target output we want our network to produce.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [3]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Show an example input and target</span>
<span class="k">def</span> <span class="nf">printSample</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Print a sample in a more visual way."""</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x1</span><span class="p">])</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">t</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'x1:   {:s}   {:2d}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x1</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'x2: + {:s}   {:2d} '</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x2</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'      -------   --'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'t:  = {:s}   {:2d}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">t</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'y:  = {:s}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
    
<span class="c"># Print the first sample</span>
<span class="n">printSample</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span>
</pre>
      </div>
     </div>
    </div>
   </div>
   <div class="output_wrapper">
    <div class="output">
     <div class="output_area">
      <div class="prompt">
      </div>
      <div class="output_subarea output_stream output_stdout output_text">
       <pre>x1:   1010010   37
x2: + 1101010   43 
      -------   --
t:  = 0000101   80
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Recurrent-neural-network-architecture">
      Recurrent neural network architecture
      <a class="anchor-link" href="#Recurrent-neural-network-architecture">
       ¶
      </a>
     </h2>
     <p>
      Our recurrent network will take 2 input variables for each sample for each timepoint, transform them to states, and output a single probability that the current output is $1$ (instead of $0$). The input is transformed into the states of the RNN where it can hold information so the network knows what to output the next timestep.
     </p>
     <p>
      There are many ways to visualise the RNN we are going to build. We can visualise the network as in the
      <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
       previous part
      </a>
      of our tutorial and unfold the processing of each input, state-update and output of a single timestep separately from the other timesteps.
     </p>
     <p>
      <img alt="Structure of the RNN" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_1.png"/>
     </p>
     <p>
      Or we can view the processing of the full input, state-updates, and full output seperately from each other. The full input tensor can be
      <a href="https://en.wikipedia.org/wiki/Map_%28higher-order_function%29">
       mapped
      </a>
      in parallel to be used directly in the RNN state updates. And also the RNN states can be mapped in parallel to the output of each timestep.
     </p>
     <p>
      <img alt="Structure of the RNN tensor processing" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_2.png"/>
     </p>
     <p>
      The steps are abstracted in different classes below. Each class has a
      <code>
       forward
      </code>
      method that performs the
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part03/#1.-Forward-step">
       forward steps
      </a>
      of backpropagation, and a
      <code>
       backward
      </code>
      method that perform the
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part03/#2.-Backward-step">
       backward
      </a>
      steps of backpropagation.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h3 id="Processing-of-input-and-output-tensors">
      Processing of input and output tensors
      <a class="anchor-link" href="#Processing-of-input-and-output-tensors">
       ¶
      </a>
     </h3>
     <h4 id="Linear-transformation">
      Linear transformation
      <a class="anchor-link" href="#Linear-transformation">
       ¶
      </a>
     </h4>
     <p>
      Neural networks typically transform input vectors by matrix multiplication and vector addition followed by a non-linear transfer function. The 2-dimensional input vectors to our network ($x_{ik1}$, $x_{ik2}$) are transformed by a $2 \times 3$ weight matrix and a bias vector of size 3. Before they can be added to the states of the RNN. The 3-dimensional state vectors are transformed to a 1-dimensional output vector by a $3 \times 1$ weight matrix and a bias vector of size 1 to give the output probabilities.
     </p>
     <p>
      Since we want to process all inputs for each sample and each timestep in one computation we can use the numpy
      <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html">
       <code>
        tensordot
       </code>
      </a>
      function to perform the dot products. This function takes 2 tensors and the axes that need to be aggregated by summation between the elements and a product of the result. For example the transformation of input X ($2000 \times 7 \times 2$) to the states S ($2000 \times 7 \times 3$) with the help of matrix W ($2 \times 3$) can be done by
      <code>
       S = tensordot(X, W, axes=((-1),(0)))
      </code>
      . This method will sum the elements of the last order (-1) of X with the elements of the first order (0) of W and multiply them together. This is the same as doing the matrix dot product for each [$x_{ik1}$, $x_{ik2}$] vector with W.
      <code>
       tensordot
      </code>
      can then make sure that the underlying computations can be done efficiently and in parallel.
     </p>
     <p>
      These linear tensor transformations are used to transform the input X to the states S, and from the states S to the output Y. This linear transformation, together with its gradient is implemented in the
      <code>
       TensorLinear
      </code>
      class below. Note that the weights are initialized by sampling uniformly between $\pm \sqrt{6.0 / (n_{in} + n_{out})}$
      <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">
       as suggested by X. Glorot
      </a>
      .
     </p>
     <h4 id="Logistic-classification">
      Logistic classification
      <a class="anchor-link" href="#Logistic-classification">
       ¶
      </a>
     </h4>
     <p>
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo01/">
       Logistic classification
      </a>
      is used to output the probability that the output at current time step k is 1. This function together with its cost and gradient is implemented in the
      <code>
       LogisticClassifier
      </code>
      class below.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [4]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define the linear tensor transformation layer</span>
<span class="k">class</span> <span class="nc">TensorLinear</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The linear tensor layer applies a linear tensor dot product and a bias to its input."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">tensor_order</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""Initialse the weight W and bias b parameters."""</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">W</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_out</span><span class="p">))</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">b</span><span class="p">)</span>  <span class="c"># Bias paramters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">tensor_order</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c"># Axes summed over in backprop</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform forward step transformation with the help of a tensor product."""</span>
        <span class="c"># Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b (for i,j in X.shape[0:1])</span>
        <span class="c"># Same as: Y = np.einsum('ijk,kl-&gt;ijl', X, self.W) + self.b</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="c"># Same as: gW = np.einsum('ijk,ijl-&gt;kl', X, gY)</span>
        <span class="c"># Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) (for i,j in X.shape[0:1])</span>
        <span class="n">gW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">))</span>
        <span class="n">gB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">)</span>
        <span class="c"># Same as: gX = np.einsum('ijk,kl-&gt;ijl', gY, self.W.T)</span>
        <span class="c"># Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) (for i,j in gY.shape[0:1])</span>
        <span class="n">gX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span>  
        <span class="k">return</span> <span class="n">gX</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [5]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define the logistic classifier layer</span>
<span class="k">class</span> <span class="nc">LogisticClassifier</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its inputs."""</span>
   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient with respect to the cost function at the inputs of this layer."""</span>
        <span class="c"># Normalise of the number of samples and sequence length.</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Compute the cost at the output."""</span>
        <span class="c"># Normalise of the number of samples and sequence length.</span>
        <span class="c"># Add a small number (1e-99) because Y can become 0 if the network learns</span>
        <span class="c">#  to perfectly predict the output. log(0) is undefined.</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="o">+</span><span class="mf">1e-99</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="o">+</span><span class="mf">1e-99</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h3 id="Unfolding-the-recurrent-states">
      Unfolding the recurrent states
      <a class="anchor-link" href="#Unfolding-the-recurrent-states">
       ¶
      </a>
     </h3>
     <p>
      Just as in the
      <a href="(http://peterroelants.github.io/posts/rnn_implementation_part01/">
       previous part
      </a>
      of this tutorial the recurrent states need to be unfolded through time. This unfolding during
      <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time">
       backpropagation through time
      </a>
      is done by the
      <code>
       RecurrentStateUnfold
      </code>
      class. This class holds the shared weight and bias parameters used to update each state, as well as the initial state that is also treated as a parameter and optimized during backpropagation.
     </p>
     <p>
      The
      <code>
       forward
      </code>
      method of
      <code>
       RecurrentStateUnfold
      </code>
      iteratively updates the states through time and returns the resulting state tensor. The
      <code>
       backward
      </code>
      method propagates the gradients at the outputs of each state backwards through time. Note that at each time $k$ the gradient coming from the output Y needs to be added with the gradient coming from the previous state at time $k+1$. The gradients of the weight and bias parameters are summed over all timestep since they are shared parameters in each state update. The final state gradient at time $k=0$ is used to optimise the initial state $S_0$ since the gradient of the inital state is $\partial \xi / \partial S_{0}$.
     </p>
     <p>
      <code>
       RecurrentStateUnfold
      </code>
      makes use of the
      <code>
       RecurrentStateUpdate
      </code>
      class. The
      <code>
       forward
      </code>
      method of this class combines the transformed input and state at time $k-1$ to output state $k$. The
      <code>
       backward
      </code>
      method propagates the gradient backwards through time for one timestep and calculates the gradients of the parameters of this timestep. The non-linear transfer function used in
      <code>
       RecurrentStateUpdate
      </code>
      is the
      <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">
       hyperbolic tangent
      </a>
      (tanh) function. This function, like the logistic function, is a
      <a href="https://en.wikipedia.org/wiki/Sigmoid_function">
       sigmoid function
      </a>
      that goes from $-1$ to $+1$. The
      <a href="https://theclevermachine.wordpress.com/tag/tanh-function/">
       tanh function
      </a>
      is chosen because the maximum gradient of this function is higher than the maximum gradient of the
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part02/#Logistic-function">
       logistic function
      </a>
      which make vanishing gradients less likely. This tanh transfer function is implemented in the
      <code>
       TanH
      </code>
      class.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [6]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define tanh layer</span>
<span class="k">class</span> <span class="nc">TanH</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""TanH applies the tanh function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="n">gTanh</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">gTanh</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [7]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define internal state update layer</span>
<span class="k">class</span> <span class="nc">RecurrentStateUpdate</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Update a given state."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">"""Initialse the linear transformation and tanh transfer function."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">TanH</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Sk</span><span class="p">):</span>
        <span class="sd">"""Return state k+1 from input and state k."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Xk</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Sk</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Sk0</span><span class="p">,</span> <span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk0</span><span class="p">,</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [8]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define layer that unfolds the states over time</span>
<span class="k">class</span> <span class="nc">RecurrentStateUnfold</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Unfold the recurrent states."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">nbTimesteps</span><span class="p">):</span>
        <span class="s">" Initialse the shared parameters, the inital state and state update function."</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">nbStates</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c"># Shared bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbStates</span><span class="p">)</span>  <span class="c"># Initial state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span> <span class="o">=</span> <span class="n">nbTimesteps</span>  <span class="c"># Timesteps to unfold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span> <span class="o">=</span> <span class="n">RecurrentStateUpdate</span><span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c"># State update function</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Iteratively apply forward step to all states."""</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c"># State tensor</span>
        <span class="n">S</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">S0</span>  <span class="c"># Set initial state</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="p">):</span>
            <span class="c"># Update the states iteratively</span>
            <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:])</span>
        <span class="k">return</span> <span class="n">S</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of this layer."""</span>
        <span class="n">gSk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gY</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,:])</span>  <span class="c"># Initialise gradient of state outputs</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Initialse gradient tensor for state inputs</span>
        <span class="n">gWSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>  <span class="c"># Initialise weight gradients</span>
        <span class="n">gBSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c"># Initialse bias gradients</span>
        <span class="c"># Propagate the gradients iteratively</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="c"># Gradient at state output is gradient from previous state plus gradient from output</span>
            <span class="n">gSk</span> <span class="o">+=</span> <span class="n">gY</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:]</span>
            <span class="c"># Propgate the gradient back through one state</span>
            <span class="n">gZ</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">)</span>
            <span class="n">gWSum</span> <span class="o">+=</span> <span class="n">gW</span>  <span class="c"># Update total weight gradient</span>
            <span class="n">gBSum</span> <span class="o">+=</span> <span class="n">gB</span>  <span class="c"># Update total bias gradient</span>
        <span class="n">gS0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gSk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># Get gradient of initial state over all samples</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gWSum</span><span class="p">,</span> <span class="n">gBSum</span><span class="p">,</span> <span class="n">gS0</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h3 id="The-full-network">
      The full network
      <a class="anchor-link" href="#The-full-network">
       ¶
      </a>
     </h3>
     <p>
      The full network that will be trained to perform binary addition of two number is defined in the
      <code>
       RnnBinaryAdder
      </code>
      class below. It initialises all the layers upon creation. The
      <code>
       forward
      </code>
      method performs the full backpropagation forward step through all layers and timesteps and returns the intermediary outputs. The
      <code>
       backward
      </code>
      method performs the backward step through all layers and timesteps and returns the gradients of all the parameters. The
      <code>
       getParamGrads
      </code>
      method performs both steps and returns the gradients of the parameters in a list. The order of this list corresponds to the order of the iterator returned by
      <code>
       get_params_iter
      </code>
      . The parameters returned in the iterator of that last method are the same as the parameters of the network and can be used to change the parameters of the network manually.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [9]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Define the full network</span>
<span class="k">class</span> <span class="nc">RnnBinaryAdder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""RNN to perform binary addition of 2 numbers."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">):</span>
        <span class="sd">"""Initialse the network layers."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c"># Input layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span> <span class="o">=</span> <span class="n">RecurrentStateUnfold</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>  <span class="c"># Recurrent layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c"># Linear output transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticClassifier</span><span class="p">()</span>  <span class="c"># Classification output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward propagation of input X through all layers."""</span>
        <span class="n">recIn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Linear input transformation</span>
        <span class="c"># Forward propagate through time and return states</span>
        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">recIn</span><span class="p">)</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:])</span>  <span class="c"># Linear output transformation</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>  <span class="c"># Get classification probabilities</span>
        <span class="c"># Return: input to recurrent layer, states, input to classifier, output</span>
        <span class="k">return</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Perform the backward propagation through all layers.</span>
<span class="sd">        Input: input samples, network output, intput to recurrent layer, states, targets."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>  <span class="c"># Get output gradient</span>
        <span class="n">gRecOut</span><span class="p">,</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="c"># Propagate gradient backwards through time</span>
        <span class="n">gRnnIn</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gRecOut</span><span class="p">)</span>
        <span class="n">gX</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gRnnIn</span><span class="p">)</span>
        <span class="c"># Return the parameter gradients of: linear output weights, linear output bias,</span>
        <span class="c">#  recursive weights, recursive bias, linear input weights, linear input bias, initial state.</span>
        <span class="k">return</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span>
    
    <span class="k">def</span> <span class="nf">getOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the output probabilities of input X."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Y</span>  <span class="c"># Only return the output.</span>
    
    <span class="k">def</span> <span class="nf">getBinaryOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the binary output of input X."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">getParamGrads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradients with respect to input X and target T as a list.</span>
<span class="sd">        The list has the same order as the get_params_iter iterator."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gS0</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWout</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBout</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the cost of input X w.r.t. targets T."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters.</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">S0</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]),</span> 
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">'readwrite'</span><span class="p">]))</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Gradient-Checking">
      Gradient Checking
      <a class="anchor-link" href="#Gradient-Checking">
       ¶
      </a>
     </h2>
     <p>
      As in
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/#Gradient-checking">
       part 4 of our previous tutorial on feedforward nets
      </a>
      the gradient computed by backpropagation is compared with the
      <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
       numerical gradient
      </a>
      to assert that there are no bugs in the code to compute the gradients. This is done by the code below.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [10]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Do gradient checking</span>
<span class="c"># Define an RNN to test</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c"># Get the gradients of the parameters from a subset of the data</span>
<span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>

<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c"># Set the small change to compute the numerical gradient</span>
<span class="c"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
    <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
    <span class="c"># + eps</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="n">plus_cost</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c"># - eps</span>
    <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="n">min_cost</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c"># reset param value</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="c"># calculate numerical gradient</span>
    <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_cost</span> <span class="o">-</span> <span class="n">min_cost</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
    <span class="c"># Raise error if the numerical grade is not close to the backprop gradient</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">'Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">grad_num</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">grad_backprop</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'No gradient errors found'</span><span class="p">)</span>
</pre>
      </div>
     </div>
    </div>
   </div>
   <div class="output_wrapper">
    <div class="output">
     <div class="output_area">
      <div class="prompt">
      </div>
      <div class="output_subarea output_stream output_stdout output_text">
       <pre>No gradient errors found
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Rmsprop-with-momentum-optimisation">
      Rmsprop with momentum optimisation
      <a class="anchor-link" href="#Rmsprop-with-momentum-optimisation">
       ¶
      </a>
     </h2>
     <p>
      While the
      <a href="http://peterroelants.github.io/posts/rnn_implementation_part01/">
       first part
      </a>
      of this tutorial used
      <a href="https://en.wikipedia.org/wiki/Rprop">
       Rprop
      </a>
      to optimise the network, this part will use the
      <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
       Rmsprop
      </a>
      algorithm with
      <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
       Nesterov's accelerated gradient
      </a>
      to perform the optimisation. We replaced the Rprop algorithm because Rprop doesn't work well with
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation">
       minibatches
      </a>
      due to the stochastic nature of the error surface that can result in sign changes of the gradient.
     </p>
     <p>
      The Rmsprop algorithm was inspired by the Rprop algorithm. It keeps a
      <a href="https://en.wikipedia.org/wiki/Moving_average">
       moving average
      </a>
      (MA) of the squared gradient for each parameter $\theta$ ($MA = \lambda * MA + (1-\lambda) * (\partial \xi / \partial \theta)^2$, with $\lambda$ the moving average hyperparameter). The gradient is then normalised by dividing by the square root of this moving average (
      <code>
       maSquare
      </code>
      = $(\partial \xi / \partial \theta)/\sqrt{MA}$). This normalised gradient is then used to update the parameters. Note that if $\lambda=0$ the gradient is reduced to its sign.
     </p>
     <p>
      This transformed gradient is not used directly to update the parameters, but it is used to update a velocity parameter (
      <code>
       Vs
      </code>
      ) for each parameter of the network. This parameter is similar to the velocity parameter from our
      <a href="http://peterroelants.github.io/posts/neural_network_implementation_part04/#Backpropagation-updates-with-momentum">
       previous tutorial
      </a>
      but it is used in a slightly different way. Nesterov's accelerated gradient is different from regular momentum in that it applies updates in a different way. While the regular momentum algorithm calculates the gradient at the beginning of the iteration, updates the velocity and moves the parameters according to this velocity, Nesterov's accelerated gradient moves the parameters according to the reduced velocity, then calculates the gradients, updates the velocity, and then moves again according to the local gradient. This has as benefit that the gradient is more informative to do the local update, and can correct for a bad velocity update. The Nesterov updates can be described as:
     </p>
     $$\begin{split}
V_{i+1} &amp; = \lambda V_i - \mu \nabla(\theta_i + \lambda V_i) \\
\theta_{i+1} &amp; = \theta_i + V_{i+1} \\
\end{split}$$
     <p>
      With $\nabla(\theta)$ the local gradient at position $\theta$ in the parameter space. And $i$ the iteration number. This formula can be visualised as in the following illustration (See
      <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
       Sutskever I.
      </a>
      ):
     </p>
     <p>
      <img alt="Illustration of Nesterov Momentum updates" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/NesterovMomentum.png"/>
     </p>
     <p>
      Note that the training converges to a cost of 0. This convergence is actually not guaranteed. If the parameters of the network start out in a bad position the network might convert to a local minimum that is far from the global minimum. The training is also sensitive to the meta parameters
      <code>
       lmbd
      </code>
      ,
      <code>
       learning_rate
      </code>
      ,
      <code>
       momentum_term
      </code>
      ,
      <code>
       eps
      </code>
      . Try rerunning this yourself to see how many times it actually converges.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [11]:
    </div>
    <div class="inner_cell">
     <div class="input_area">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Set hyper-parameters</span>
<span class="n">lmbd</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c"># Rmsprop lambda</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c"># Learning rate</span>
<span class="n">momentum_term</span> <span class="o">=</span> <span class="mf">0.80</span>  <span class="c"># Momentum term</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>  <span class="c"># Numerical stability term to prevent division by zero</span>
<span class="n">mb_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c"># Size of the minibatches (number of samples)</span>

<span class="c"># Create the network</span>
<span class="n">nb_of_states</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c"># Number of states in the recurrent layer</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c"># Set the initial parameters</span>
<span class="n">nbParameters</span> <span class="o">=</span>  <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">())</span>  <span class="c"># Number of parameters in the network</span>
<span class="n">maSquare</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>  <span class="c"># Rmsprop moving average</span>
<span class="n">Vs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>  <span class="c"># Velocity</span>

<span class="c"># Create a list of minibatch costs to be plotted</span>
<span class="n">ls_of_costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])]</span>
<span class="c"># Iterate over some iterations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="c"># Iterate over all the minibatches</span>
    <span class="k">for</span> <span class="n">mb</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_train</span><span class="o">/</span><span class="n">mb_size</span><span class="p">):</span>
        <span class="n">X_mb</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c"># Input minibatch</span>
        <span class="n">T_mb</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c"># Target minibatch</span>
        <span class="n">V_tmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="o">*</span> <span class="n">momentum_term</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Vs</span><span class="p">]</span>
        <span class="c"># Update each parameters according to previous gradient</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="n">P</span> <span class="o">+=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span>
        <span class="c"># Get gradients after following old velocity</span>
        <span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span><span class="n">X_mb</span><span class="p">,</span> <span class="n">T_mb</span><span class="p">)</span>  <span class="c"># Get the parameter gradients    </span>
        <span class="c"># Update each parameter seperately</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="c"># Update the Rmsprop moving averages</span>
            <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmbd</span> <span class="o">*</span> <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">lmbd</span><span class="p">)</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
            <span class="c"># Calculate the Rmsprop normalised gradient</span>
            <span class="n">pGradNorm</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="c"># Update the momentum velocity</span>
            <span class="n">Vs</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">-</span> <span class="n">pGradNorm</span>     
            <span class="n">P</span> <span class="o">-=</span> <span class="n">pGradNorm</span>   <span class="c"># Update the parameter</span>
        <span class="n">ls_of_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_mb</span><span class="p">),</span> <span class="n">T_mb</span><span class="p">))</span>  <span class="c"># Add cost to list to plot</span>
</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [12]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Plot the cost over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ls_of_costs</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'minibatch iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$</span><span class="se">\\</span><span class="s">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decrease of cost over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
      </div>
     </div>
    </div>
   </div>
   <div class="output_wrapper">
    <div class="output">
     <div class="output_area">
      <div class="prompt">
      </div>
      <div class="output_png output_subarea ">
       <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAY8AAAEZCAYAAABvpam5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <h2 id="Test-examples">
      Test examples
      <a class="anchor-link" href="#Test-examples">
       ¶
      </a>
     </h2>
     <p>
      The figure above shows that the training converged to a cost of 0. We expect the network to have learned how to perfectly do binary addition for our training examples. If we put some independent test cases through the network and print them out we can see that the network also outputs the correct output for these test cases.
     </p>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing code_cell rendered">
   <div class="input">
    <div class="prompt input_prompt">
     In [13]:
    </div>
    <div class="inner_cell">
     <div class="input_area collapsed">
      <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
      </div>
      <div class=" highlight hl-ipython2">
       <pre><span class="c"># Create test samples</span>
<span class="n">nb_test</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">Xtest</span><span class="p">,</span> <span class="n">Ttest</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_test</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c"># Push test data through network</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getBinaryOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
<span class="n">Yf</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>

<span class="c"># Print out all test examples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xtest</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">printSample</span><span class="p">(</span><span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">Ttest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="k">print</span> <span class="s">''</span>
</pre>
      </div>
     </div>
    </div>
   </div>
   <div class="output_wrapper">
    <div class="output">
     <div class="output_area">
      <div class="prompt">
      </div>
      <div class="output_subarea output_stream output_stdout output_text">
       <pre>x1:   0100010   34
x2: + 1100100   19 
      -------   --
t:  = 1010110   53
y:  = 1010110

x1:   1010100   21
x2: + 1110100   23 
      -------   --
t:  = 0011010   44
y:  = 0011010

x1:   1111010   47
x2: + 0000000    0 
      -------   --
t:  = 1111010   47
y:  = 1111010

x1:   1000000    1
x2: + 1111110   63 
      -------   --
t:  = 0000001   64
y:  = 0000001

x1:   1010100   21
x2: + 1010100   21 
      -------   --
t:  = 0101010   42
y:  = 0101010

</pre>
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="cell border-box-sizing text_cell rendered">
   <div class="prompt input_prompt">
   </div>
   <div class="inner_cell">
    <div class="text_cell_render border-box-sizing rendered_html">
     <p>
      This post at
      <a href="http://peterroelants.github.io/posts/rnn_implementation_part02/">
       peterroelants.github.io
      </a>
      is generated from an IPython notebook file.
      <a href="https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn_implementation_part02.ipynb">
       Link to the full IPython notebook file
      </a>
     </p>
    </div>
   </div>
  </div>
 </body>
</html>
